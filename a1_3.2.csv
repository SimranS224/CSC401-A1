"0.3781254178366092","0.40700628426260194","0.424655702634042","0.43080625752105894","0.44858938360743417"
We expect the accuracy to increase as the number of training sample increases. This is because of the way Adaboost works, since it keeps assigning higher weight to the wrong classified observations on each iteration , and since it keeps repeating this until the it has reached the maximum number of estimators, we can argue that if there is more data it will be more accurate. But with one caveat due to the fact that Adaboost is sensitive to noise - if the data we have has a lot of noise it can be affected by outliers. Similarly, other classifiers can also become more accurate with more data, however there are two reasons why more data might not improve accuracy and they are if the model is too simple resulting in high bias or if the model is too complex i.e uses too many features. However in this case more data and fewer features will resolve this problem.
